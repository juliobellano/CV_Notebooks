{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliobellano/CV_Notebooks/blob/main/HuggingPics2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFglcc3TASu"
      },
      "source": [
        "# HuggingPics ü§óüñºÔ∏è\n",
        "\n",
        "Fine-tune Vision Transformers for **anything** using images found on the web."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVyU3ScYZ3Hc"
      },
      "source": [
        "%%capture\n",
        "\n",
        "! pip install transformers pytorch-lightning --quiet\n",
        "! sudo apt -qq install git-lfs\n",
        "! git config --global credential.helper store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihFempth1zK0"
      },
      "source": [
        "import requests\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "from getpass import getpass\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from requests.exceptions import HTTPError\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision.datasets import ImageFolder\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utxw4wj-RS94"
      },
      "source": [
        "## Defining your search terms\n",
        "\n",
        "Simply replace the terms in the widget below with whatever you want to classify, and we'll download ~150 images for each class for you.\n",
        "\n",
        "You can define up to 5 classes. For < 5 classes, just leave the remaining text boxes blank.\n",
        "\n",
        "### Examples\n",
        "\n",
        "üí° If you need some inspiration, take a look at these examples:\n",
        "\n",
        "|            | [nateraw/rare-puppers](https://huggingface.co/nateraw/rare-puppers) | [nateraw/pasta-pizza-ravioli](https://huggingface.co/nateraw/pasta-pizza-ravioli) | [nateraw/baseball-stadium-foods](https://huggingface.co/nateraw/baseball-stadium-foods) | [nateraw/denver-nyc-paris](https://huggingface.co/nateraw/denver-nyc-paris) |\n",
        "| ---------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| **term_1** | samoyed                                                             | pizza                                                                             | cotton candy                                                                            | denver                                                                      |\n",
        "| **term_2** | shiba inu                                                           | pasta                                                                             | hamburger                                                                               | new york city                                                               |\n",
        "| **term_3** | corgi                                                               | ravioli                                                                           | hot dog                                                                                 | paris                                                                       |\n",
        "| **term_4** |                                                                     |                                                                                   | nachos                                                                                  |                                                                             |\n",
        "| **term_5** |                                                                     |                                                                                   | popcorn                                                                                 |                                                                             |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGyijnL08Cge",
        "cellView": "form"
      },
      "source": [
        "term_1 = \"samoyed\" #@param {type:\"string\"}\n",
        "term_2 = \"shiba inu\" #@param {type:\"string\"}\n",
        "term_3 = \"corgi\" #@param {type:\"string\"}\n",
        "term_4 = \"\" #@param {type:\"string\"}\n",
        "term_5 = \"\" #@param {type:\"string\"}\n",
        "\n",
        "search_terms = sorted([\n",
        "    term_1,\n",
        "    term_2,\n",
        "    term_3,\n",
        "    term_4,\n",
        "    term_5\n",
        "])\n",
        "\n",
        "search_terms = [x for x in search_terms if x.strip() != '']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O-Z8-k3DByn"
      },
      "source": [
        "## Get Images From The Web\n",
        "\n",
        "Here, we loop over your selected search terms and collect ~150 related images for each.\n",
        "\n",
        "We'll save them to a new folder named `images/` that is structured to work with [`torchvision.datasets.ImageFolder`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageFolder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEH5b3rsdId"
      },
      "source": [
        "SEARCH_URL = \"https://huggingpics-api-server.fly.dev/images/search\"\n",
        "\n",
        "def get_image_urls_by_term(search_term: str, count=150):\n",
        "    params  = {\"q\": search_term, \"license\": \"public\", \"imageType\": \"photo\", \"count\": count}\n",
        "    response = requests.get(SEARCH_URL, params=params)\n",
        "    response.raise_for_status()\n",
        "    response_data = response.json()\n",
        "    image_urls = [img['thumbnailUrl'] for img in response_data['value']]\n",
        "    return image_urls\n",
        "\n",
        "\n",
        "def gen_images_from_urls(urls):\n",
        "    num_skipped = 0\n",
        "    for url in urls:\n",
        "        response = requests.get(url)\n",
        "        if not response.status_code == 200:\n",
        "            num_skipped += 1\n",
        "        try:\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            yield img\n",
        "        except UnidentifiedImageError:\n",
        "            num_skipped +=1\n",
        "\n",
        "    print(f\"Retrieved {len(urls) - num_skipped} images. Skipped {num_skipped}.\")\n",
        "\n",
        "\n",
        "def urls_to_image_folder(urls, save_directory):\n",
        "    for i, image in enumerate(gen_images_from_urls(urls)):\n",
        "        image.save(save_directory / f'{i}.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZoWowITeoVTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/input.zip /content/input.zip"
      ],
      "metadata": {
        "id": "mjbsgZmqsOKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip input.zip -d /content/input"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s28ZXk7QsZFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKBb34N9qnNS"
      },
      "source": [
        "data_dir = Path('input/correlation_assignment/images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_r5KO0aMIsn"
      },
      "source": [
        "## Init Dataset and Split into Training and Validation Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4vCcep26Em6"
      },
      "source": [
        "#ds = ImageFolder(data_dir)\n",
        "#indices = torch.randperm(len(ds)).tolist()\n",
        "#n_val = math.floor(len(indices) * .15)\n",
        "#train_ds = torch.utils.data.Subset(ds, indices[:-n_val])\n",
        "#val_ds = torch.utils.data.Subset(ds, indices[-n_val:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('input/correlation_assignment/responses.csv')\n",
        "\n",
        "image_id = df['id'].values\n",
        "labels = df['corr'].values\n",
        "print(len(image_id))"
      ],
      "metadata": {
        "id": "M8GNu0cAt3lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Normalize((0.5,),(0.5,))])\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, labels, image_dir='input/correlation_assignment/images', transform=None):\n",
        "        self.image_ids = image_ids\n",
        "        self.labels = labels\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #get image pathhh\n",
        "        img_name = self.image_ids[idx]\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_name}.png\")\n",
        "\n",
        "        #load image\n",
        "        image = Image.open(img_path).convert('1')\n",
        "\n",
        "        #apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        #get label\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "lkgTFHPqyC24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(image_id, labels, test_size=0.15, random_state=42, shuffle=True)\n",
        "print(f\"length of training dataset {len(X_train)}\")\n",
        "print(f\"length of val dataset {len(X_val)}\")\n",
        "\n",
        "train_ds = ImageDataset(X_train, y_train, transform = data_transforms)\n",
        "val_ds = ImageDataset(X_val, y_val, transform = data_transforms)"
      ],
      "metadata": {
        "id": "kv_PmdGdte6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABreTsYRoDzy"
      },
      "source": [
        "## Show Some Examples\n",
        "\n",
        "#### ‚ö†Ô∏è Note - The image search API isn't perfect ‚ö†Ô∏è\n",
        "\n",
        "You may need to go back, tweak your search terms, and repeat the cells above until the images shown below look good.\n",
        "\n",
        "A few bad images is OK, but if they are completely incorrect, you'll definitely want to try again with different terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbzsRQRe6iY5"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.title(f'{y_train[i]}')\n",
        "    img = plt.imread(f'/content/input/correlation_assignment/images/{X_train[i]}.png')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96LqcgYfLGc8"
      },
      "source": [
        "## Preparing Labels for Our Model's Config\n",
        "\n",
        "By adding `label2id` + `id2label` to our model's config, we'll get friendlier labels in the inference API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh0yxRKDGDd5"
      },
      "source": [
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for i, class_name in enumerate(ds.classes):\n",
        "    label2id[class_name] = str(i)\n",
        "    id2label[str(i)] = class_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1aly6Vrx2mM"
      },
      "source": [
        "## Image Classification Collator\n",
        "\n",
        "To apply our transforms to images, we'll use a custom collator class. We'll initialize it using an instance of `ViTFeatureExtractor` and pass the collator instance to `torch.utils.data.DataLoader`'s `collate_fn` kwarg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2n0Mf1u1ymf"
      },
      "source": [
        "class ImageClassificationCollator:\n",
        "    def __init__(self, feature_extractor):\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        processed_images = []\n",
        "        for x in batch:\n",
        "            img = x[0]\n",
        "\n",
        "            # Convert from [-1,1] to [0,255] range\n",
        "            img = ((img + 1) * 127.5).byte()\n",
        "\n",
        "            # Convert grayscale to RGB using PIL\n",
        "            # This works better than just repeating channels\n",
        "            if img.shape[0] == 1:  # If single channel\n",
        "                # Convert tensor to PIL\n",
        "                pil_img = Image.fromarray(img[0].numpy())\n",
        "                # Convert to RGB\n",
        "                pil_img = pil_img.convert('RGB')\n",
        "                processed_images.append(pil_img)\n",
        "            else:\n",
        "                processed_images.append(img)\n",
        "\n",
        "        encodings = self.feature_extractor(processed_images, return_tensors='pt')\n",
        "        encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.float)\n",
        "        return encodings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T62uFtK7LTcz"
      },
      "source": [
        "## Init Feature Extractor, Model, Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YovQELKD4Bu8"
      },
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=1,\n",
        "    problem_type='regression'\n",
        ")\n",
        "\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(model.classifier.in_features, 1),\n",
        "    torch.nn.Tanh()\n",
        ")\n",
        "\n",
        "\n",
        "collator = ImageClassificationCollator(feature_extractor)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, collate_fn=collator, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, collate_fn=collator, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgf32rC7pQh"
      },
      "source": [
        "# Training\n",
        "\n",
        "‚ö° We'll use [PyTorch Lightning](https://pytorchlightning.ai/) to fine-tune our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIIRpEzW4LFo"
      },
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model, lr: float = 2e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters('lr', *list(kwargs))\n",
        "        self.model = model\n",
        "        self.forward = self.model.forward\n",
        "        self.mse = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        loss = self.mse(outputs.logits.squeeze(), batch['labels'])\n",
        "        self.log(f\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        preds = outputs.logits.squeeze()\n",
        "        loss = self.mse(preds, batch['labels'])\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTWYA1rf3Heg"
      },
      "source": [
        "pl.seed_everything(42)\n",
        "classifier = Classifier(model, lr=2e-5)\n",
        "trainer = pl.Trainer(accelerator='gpu', devices=1, precision=16, max_epochs=1)\n",
        "trainer.fit(classifier, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJLxkmefLZT0"
      },
      "source": [
        "## Check if it Worked üòÖ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDfUUwH73LSq"
      },
      "source": [
        "val_batch = next(iter(val_loader))\n",
        "outputs = model(**val_batch)\n",
        "print('Preds: ', outputs.logits.squeeze())\n",
        "print('Labels:', val_batch['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_checkpoint(\"full_model.ckpt\")"
      ],
      "metadata": {
        "id": "EYMDdW3t5sfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FlEoisI9S-e"
      },
      "source": [
        "# Push to ü§ó Hub üöÄ\n",
        "\n",
        "You'll need to authenticate with your Hugging Face account, so make sure to [sign up](https://huggingface.co/join) if you haven't already.\n",
        "\n",
        "Your repo will be created at `{hf_username}/{model_id}`, so pick a `model_id` you like üòä"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "d1t_A7PkNRiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t58j9AN-iELM",
        "cellView": "form"
      },
      "source": [
        "model_id = \"rare-puppers\" #@param {type:\"string\"}\n",
        "\n",
        "description = \"\"\"\n",
        "Autogenerated by HuggingPicsü§óüñºÔ∏è\n",
        "\n",
        "Create your own image classifier for **anything** by running [the demo on Google Colab](https://colab.research.google.com/github/nateraw/huggingpics/blob/main/HuggingPics.ipynb).\n",
        "\n",
        "Report any issues with the demo at the [github repo](https://github.com/nateraw/huggingpics).\n",
        "\"\"\"\n",
        "task_name = \"Image Classification\"\n",
        "task_type = 'image-classification'\n",
        "metric_name = 'Accuracy'\n",
        "metric_type = 'accuracy'\n",
        "metric_value = trainer.callback_metrics['val_acc'].item()\n",
        "\n",
        "# Delete model folder, as we (re)create it here.\n",
        "if Path('./model').exists():\n",
        "    shutil.rmtree('./model')\n",
        "\n",
        "token = HfFolder().get_token()\n",
        "if not token:\n",
        "    raise RuntimeError(\"You must log in to push to hub! Run notebook_login() in the cell above.\")\n",
        "\n",
        "hf_username = HfApi().whoami()['name']\n",
        "model_url = HfApi().create_repo(token=token, repo_id=model_id, exist_ok=True)\n",
        "model_repo = Repository(\"./model\", clone_from=model_url, use_auth_token=token, git_email=f\"{hf_username}@users.noreply.huggingface.co\", git_user=hf_username)\n",
        "model.save_pretrained(model_repo.local_dir)\n",
        "feature_extractor.save_pretrained(model_repo.local_dir)\n",
        "\n",
        "# Copy over tensorboard logs from lightning_logs/ into ./model/runs/\n",
        "tensorboard_logs_path = next(Path(trainer.logger.log_dir).glob('events.out*'))\n",
        "model_repo_logs_path = Path(model_repo.local_dir) / 'runs'\n",
        "model_repo_logs_path.mkdir(exist_ok=True, parents=True)\n",
        "shutil.copy(tensorboard_logs_path, model_repo_logs_path)\n",
        "\n",
        "# Copy over a few example images\n",
        "example_images_dir = Path(model_repo.local_dir) / 'images'\n",
        "example_images_dir.mkdir(exist_ok=True, parents=True)\n",
        "image_markdown_template = '''\n",
        "#### {class_name}\n",
        "\n",
        "![{class_name}](images/{example_image_path})\n",
        "'''\n",
        "example_images_markdown = \"\"\n",
        "for class_idx, class_name in enumerate(ds.classes):\n",
        "    folder = ds.root / class_name\n",
        "    image_path = sorted(folder.glob('*'))[0]\n",
        "    example_image_path = example_images_dir / f\"{class_name.replace(' ', '_')}{image_path.suffix}\"\n",
        "    shutil.copy(image_path, example_image_path)\n",
        "    example_images_markdown += image_markdown_template.format(\n",
        "        class_name=class_name,\n",
        "        example_image_path=example_image_path.name\n",
        "    )\n",
        "\n",
        "\n",
        "# Prepare README.md from information gathered above\n",
        "readme_txt = f\"\"\"\n",
        "---\n",
        "tags:\n",
        "- image-classification\n",
        "- pytorch\n",
        "- huggingpics\n",
        "metrics:\n",
        "- {metric_type}\n",
        "\n",
        "model-index:\n",
        "- name: {model_id}\n",
        "  results:\n",
        "  - task:\n",
        "      name: {task_name}\n",
        "      type: {task_type}\n",
        "    metrics:\n",
        "      - name: {metric_name}\n",
        "        type: {metric_type}\n",
        "        value: {metric_value}\n",
        "---\n",
        "\n",
        "# {model_id}\n",
        "\n",
        "{description}\n",
        "\n",
        "## Example Images\n",
        "\n",
        "{example_images_markdown}\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "(Path(model_repo.local_dir) / 'README.md').write_text(readme_txt)\n",
        "\n",
        "commit_url = model_repo.push_to_hub()\n",
        "\n",
        "print(\"Check out your model at:\")\n",
        "print(f\"https://huggingface.co/{hf_username}/{model_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}